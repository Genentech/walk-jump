import torch
import re
from sklearn.preprocessing import LabelEncoder


def tokenize_string(string: str, alphabet: list[str]) -> list[str]:
    """
    Tokenize a string element of alphabet* into a list.

    Parameters
    ----------
    string: str
        Element of the language alphabet*, i.e.,
        it is a string of any length composed of a known set of characters.
    alphabet: List[str]
        The fixed token set

    Returns
    -------
    List[str]
        Tokenized version of the input
    """
    escaped_alphabet = [re.escape(tok) for tok in alphabet]
    regex = "|".join(escaped_alphabet)
    return re.findall(regex, string)


def token_string_to_tensor(
    string: str, alphabet: LabelEncoder, onehot: bool = False
) -> torch.Tensor:
    tokenized = tokenize_string(string, alphabet.classes_.tolist())
    tensor = torch.from_numpy(alphabet.transform(tokenized)).long()

    if onehot:
        size = len(alphabet.classes_)
        tensor = torch.nn.functional.one_hot(tensor, num_classes=size)
    return tensor


def token_string_from_tensor(
    tensor: torch.Tensor,
    alphabet: LabelEncoder,
    from_logits: bool = True,
) -> list[str]:
    """Convert tensor representation of sequence to list of string

    Parameters
    ----------
    tensor: torch.Tensor
        Input tensor
    from_logits: bool
        If True, elect to first compute the argmax in the final dimension of the tensor

    Returns
    -------
    List[str]
        The sequence version
    """
    # convert to shape (b, L, V) (if from_logits)  or (b, L) (if not from_logits) if necessary
    if (from_logits and tensor.dim() == 2) or (not from_logits and tensor.dim() == 1):
        tensor = tensor.unsqueeze(0)

    if from_logits:
        tensor = tensor.argmax(-1)

    tensor = tensor.cpu()
    return [
        "".join(alphabet.inverse_transform(tensor[i, ...].tolist()).tolist())
        for i in range(tensor.size(0))
    ]
